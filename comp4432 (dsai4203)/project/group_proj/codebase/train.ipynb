{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T11:54:13.032603Z",
     "start_time": "2024-12-21T11:54:12.073694Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from autoencoder import Autoencoder, VariationalAutoencoder\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21442b1470779a93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T11:54:13.042794Z",
     "start_time": "2024-12-21T11:54:13.040464Z"
    }
   },
   "outputs": [],
   "source": [
    "TEST_SIZE = 10000\n",
    "CHUNK_SIZE = 1000  # when reading from disk\n",
    "NUM_WORKERS = os.cpu_count() if os.cpu_count() is not None else 4  # Number of CPU cores\n",
    "\n",
    "# dataset class\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa43589b2e0cb2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T11:54:13.088370Z",
     "start_time": "2024-12-21T11:54:13.084476Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem = process.memory_info().rss / (1024 ** 3)\n",
    "    return mem\n",
    "\n",
    "def get_data(path):\n",
    "    # --- Read Metadata ---\n",
    "    \n",
    "    drop_cols = ['publish_date', 'headline_text']\n",
    "    tmp_df = pd.read_csv(path, nrows=0)\n",
    "    all_cols = tmp_df.columns.tolist()\n",
    "    del tmp_df\n",
    "    \n",
    "    use_cols = [col for col in all_cols if col not in drop_cols]\n",
    "    dtype_dict = {col: 'float32' for col in use_cols}\n",
    "    del drop_cols, all_cols\n",
    "    \n",
    "    print(f'Reading: {path}')\n",
    "    print('Estimating # of rows...')\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        total_rows = sum(1 for _ in f) - 1  # Subtract 1 for header\n",
    "    print(f'Total rows: {total_rows}')\n",
    "\n",
    "\n",
    "\n",
    "    # --- Read Data ---\n",
    "    \n",
    "    data_np = np.empty((total_rows, len(use_cols)), dtype='float32')\n",
    "    mem_before = get_memory_usage()\n",
    "\n",
    "    print(f'Memory Before Read: {mem_before:.2f} GB')\n",
    "    print('Reading and processing csv...')\n",
    "\n",
    "    with pd.read_csv(path, chunksize=CHUNK_SIZE, usecols=use_cols, dtype=dtype_dict) as reader:\n",
    "        for i, chunk in enumerate(tqdm(reader, total=(total_rows // CHUNK_SIZE) + 1, desc='processing chunks')):\n",
    "            start_idx = i * CHUNK_SIZE\n",
    "            end_idx = start_idx + len(chunk)\n",
    "            data_np[start_idx:end_idx] = chunk.values\n",
    "            del chunk\n",
    "\n",
    "\n",
    "    mem_after = get_memory_usage()\n",
    "    print(f'Memory After Read: {mem_after:.2f} GB')\n",
    "\n",
    "    return data_np"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ded97c1dc54afb15",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae98521d886e943",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T12:07:22.006967Z",
     "start_time": "2024-12-21T11:54:13.134072Z"
    }
   },
   "outputs": [],
   "source": [
    "input_path = 'abcnews-date-text-embed-4096.csv'\n",
    "\n",
    "data_np = get_data(input_path)\n",
    "print(f'Data shape: {data_np.shape}')\n",
    "\n",
    "np.random.shuffle(data_np)\n",
    "print('Data shuffled')\n",
    "\n",
    "train_data_np = data_np[:-TEST_SIZE]\n",
    "test_data_np = data_np[-TEST_SIZE:]\n",
    "del data_np  # Free mem\n",
    "\n",
    "print(f'Training data shape: {train_data_np.shape}')\n",
    "print(f'Testing data shape: {test_data_np.shape}')\n",
    "\n",
    "train_tensor = torch.from_numpy(train_data_np)\n",
    "test_tensor = torch.from_numpy(test_data_np)\n",
    "del train_data_np\n",
    "del test_data_np  # Free mem\n",
    "\n",
    "train_dataset = EmbeddingDataset(train_tensor)\n",
    "test_dataset = EmbeddingDataset(test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa90d3800f9ddd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T12:07:22.988087Z",
     "start_time": "2024-12-21T12:07:22.021928Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_tensor[:10])\n",
    "assert not torch.isnan(train_tensor).any().item()\n",
    "assert not torch.isnan(test_tensor).any().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566b4c8bc8315103",
   "metadata": {},
   "source": [
    "Create dataloaders (change batch size here to prevent reading from disk again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4011bcc546045abf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T12:07:22.995416Z",
     "start_time": "2024-12-21T12:07:22.993470Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024  # when training\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df1d607526520dbb",
   "metadata": {},
   "source": [
    "# Define hyperparameter and training procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfaad4ce016827a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T12:07:23.046435Z",
     "start_time": "2024-12-21T12:07:23.035040Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparam\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_dim = 4096\n",
    "hidden_dim = [2048, 1024, 256, 128]\n",
    "learning_rate = 0.001\n",
    "encoding_dim = 10\n",
    "dropout = 0.2\n",
    "epochs = 100\n",
    "\n",
    "# paths\n",
    "hidden_dim_str = '_'.join(map(str, hidden_dim))\n",
    "extra_note = '_linear_final'\n",
    "# extra_note = ''\n",
    "model_path = f'./models/autoencoder_{input_dim}_{hidden_dim_str}_{encoding_dim}_d{int(dropout * 10.0)}{extra_note}.pth'\n",
    "stats_path = f'./stats/autoencoder_{input_dim}_{hidden_dim_str}_{encoding_dim}_d{int(dropout * 10.0)}{extra_note}.csv'\n",
    "output_path = f'./latent/latent_{input_dim}_{hidden_dim_str}_{encoding_dim}_d{int(dropout * 10.0)}{extra_note}.csv'\n",
    "\n",
    "def train(model, train_dataloader, test_dataloader, crt, opt):\n",
    "    tr_loss_lst = []\n",
    "    te_loss_lst = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch}/{epochs}', leave=False)\n",
    "        for train_batch in progress_bar:\n",
    "            train_batch = train_batch.to(device, non_blocking=True)\n",
    "\n",
    "            # Forward\n",
    "            train_outputs = model(train_batch)\n",
    "            loss = crt(train_outputs, train_batch)\n",
    "\n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            epoch_loss += loss.item() * train_batch.size(0)\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0.0\n",
    "            for test_batch in test_dataloader:\n",
    "                test_batch = test_batch.to(device, non_blocking=True)\n",
    "                test_outputs = model(test_batch)\n",
    "                test_loss += crt(test_outputs, test_batch) * test_batch.size(0)\n",
    "                \n",
    "            test_loss /= len(test_dataloader.dataset)\n",
    "            te_loss_lst.append(test_loss.item())\n",
    "\n",
    "        train_loss = epoch_loss / len(train_dataloader.dataset)\n",
    "        tr_loss_lst.append(train_loss)\n",
    "        print(f'Epoch [{epoch}/{epochs}], Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}')\n",
    "\n",
    "    return tr_loss_lst, te_loss_lst\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "68b54c2810a24869",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5260bab2336c33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T12:23:32.872031Z",
     "start_time": "2024-12-21T12:07:23.089644Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "ae = Autoencoder(input_dim, hidden_dim, encoding_dim, dropout=dropout).to(device)\n",
    "# ae = VariationalAutoencoder(input_dim, hidden_dim, encoding_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loss_lst, test_loss_lst = train(ae, train_loader, test_loader, criterion, optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35f51c682247b78c",
   "metadata": {},
   "source": [
    "# save results (model, stats, latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e515769b89fc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T12:23:32.880241Z",
     "start_time": "2024-12-21T12:23:32.878633Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(train_loss_lst))\n",
    "print(len(test_loss_lst))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72da3dc67b07ed9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T12:32:57.825697Z",
     "start_time": "2024-12-21T12:23:32.923436Z"
    }
   },
   "outputs": [],
   "source": [
    "# save stats\n",
    "stats_df = pd.DataFrame({'train_loss': train_loss_lst, 'test_loss': test_loss_lst})\n",
    "stats_df.to_csv(stats_path, index=False)\n",
    "print(f'Stats saved to: {stats_path}')\n",
    "\n",
    "# save model\n",
    "torch.save(ae.state_dict(), model_path)\n",
    "print(f'Model saved to: {model_path}')\n",
    "\n",
    "# save latent\n",
    "metadata_columns = ['publish_date', 'headline_text']\n",
    "first_chunk = True\n",
    "\n",
    "print(f\"Estimating total number of rows in {input_path}...\")\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    total_rows = sum(1 for _ in f) - 1  # Subtract 1 for header\n",
    "print(f\"Total rows to process: {total_rows}\")\n",
    "\n",
    "reader = pd.read_csv(input_path, chunksize=CHUNK_SIZE)\n",
    "\n",
    "ae.eval()\n",
    "write_mode = 'w'\n",
    "for chunk in tqdm(reader, total=(total_rows // CHUNK_SIZE) + 1, desc='Processing Chunks'):\n",
    "    metadata = chunk[metadata_columns].copy().reset_index(drop=True)\n",
    "    embeddings = chunk.drop(columns=metadata_columns)\n",
    "    embeddings_np = embeddings.values.astype('float32')\n",
    "    embeddings_tensor = torch.from_numpy(embeddings_np).to(device)\n",
    "    assert not embeddings.isnull().values.any()\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        latent_tensor: torch.tensor = ae.encode(embeddings_tensor)\n",
    "\n",
    "    assert not torch.isnan(latent_tensor).any().item()\n",
    "    \n",
    "    # assert not all zero\n",
    "    assert not torch.all(latent_tensor == 0).item()\n",
    "    \n",
    "    # back to cpu and to numpy\n",
    "    assert latent_tensor.shape[1] == encoding_dim\n",
    "    \n",
    "    latent_np = latent_tensor.cpu().numpy()\n",
    "    \n",
    "    assert not np.isnan(latent_np).any()\n",
    "    \n",
    "    latent_df = pd.DataFrame(\n",
    "        latent_np, \n",
    "        columns=[str(i) for i in range(1, encoding_dim + 1)]\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    assert not pd.isna(latent_df).to_numpy().any()\n",
    "    \n",
    "    output_chunk = pd.concat([metadata, latent_df], axis=1)\n",
    "    \n",
    "    assert not pd.isna(output_chunk).to_numpy().any()\n",
    "\n",
    "    output_chunk.to_csv(\n",
    "        output_path, \n",
    "        index=False, \n",
    "        mode=write_mode, \n",
    "        header=(write_mode == 'w')\n",
    "    )\n",
    "    del output_chunk\n",
    "    \n",
    "    write_mode = 'a'\n",
    "\n",
    "print(f\"Latent representations saved to {output_path}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f9daac88a9c1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T12:32:57.876184Z",
     "start_time": "2024-12-21T12:32:57.866387Z"
    }
   },
   "outputs": [],
   "source": [
    "# print parameters of `ae` layer by layer\n",
    "for name, param in ae.named_parameters():\n",
    "    print(name, param.size())\n",
    "    print(param)\n",
    "    print('\\n\\n')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

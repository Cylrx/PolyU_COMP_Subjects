
@inproceedings{caruana_ensemble_2004,
	address = {Banff, Alberta, Canada},
	title = {Ensemble selection from libraries of models},
	copyright = {https://www.acm.org/publications/policies/copyright\_policy\#Background},
	url = {http://portal.acm.org/citation.cfm?doid=1015330.1015432},
	doi = {10.1145/1015330.1015432},
	abstract = {We present a method for constructing ensembles from libraries of thousands of models. Model libraries are generated using diﬀerent learning algorithms and parameter settings. Forward stepwise selection is used to add to the ensemble the models that maximize its performance. Ensemble selection allows ensembles to be optimized to performance metric such as accuracy, cross entropy, mean precision, or ROC Area. Experiments with seven test problems and ten metrics demonstrate the beneﬁt of ensemble selection.},
	language = {en},
	urldate = {2025-11-11},
	booktitle = {Twenty-first international conference on {Machine} learning  - {ICML} '04},
	publisher = {ACM Press},
	author = {Caruana, Rich and Niculescu-Mizil, Alexandru and Crew, Geoff and Ksikes, Alex},
	year = {2004},
	note = {TLDR: A method for constructing ensembles from libraries of thousands of models using forward stepwise selection to be optimized to performance metric such as accuracy, cross entropy, mean precision, or ROC Area is presented.},
	pages = {18},
	file = {PDF:/Users/tonywang/Zotero/storage/4RUTUAV3/Caruana et al. - 2004 - Ensemble selection from libraries of models.pdf:application/pdf},
}

@misc{salinas_tabrepo_2024,
	title = {{TabRepo}: {A} {Large} {Scale} {Repository} of {Tabular} {Model} {Evaluations} and its {AutoML} {Applications}},
	shorttitle = {{TabRepo}},
	url = {http://arxiv.org/abs/2311.02971},
	doi = {10.48550/arXiv.2311.02971},
	abstract = {We introduce TabRepo, a new dataset of tabular model evaluations and predictions. TabRepo contains the predictions and metrics of 1310 models evaluated on 200 classification and regression datasets. We illustrate the benefit of our dataset in multiple ways. First, we show that it allows to perform analysis such as comparing Hyperparameter Optimization against current AutoML systems while also considering ensembling at marginal cost by using precomputed model predictions. Second, we show that our dataset can be readily leveraged to perform transfer-learning. In particular, we show that applying standard transfer-learning techniques allows to outperform current state-of-the-art tabular systems in accuracy, runtime and latency.},
	urldate = {2025-11-12},
	publisher = {arXiv},
	author = {Salinas, David and Erickson, Nick},
	month = aug,
	year = {2024},
	note = {arXiv:2311.02971 [cs]
TLDR: TabRepo, a new dataset of tabular model evaluations and predictions, is introduced and it is shown that applying standard transfer-learning techniques allows to outperform current state-of-the-art tabular systems in accuracy, runtime and latency.},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Full Text PDF:/Users/tonywang/Zotero/storage/LRFTTQIE/Salinas and Erickson - 2024 - TabRepo A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications.pdf:application/pdf;Snapshot:/Users/tonywang/Zotero/storage/S6DUWF9F/2311.html:text/html},
}

@misc{erickson_tabarena_2025,
	title = {{TabArena}: {A} {Living} {Benchmark} for {Machine} {Learning} on {Tabular} {Data}},
	shorttitle = {{TabArena}},
	url = {http://arxiv.org/abs/2506.16791},
	doi = {10.48550/arXiv.2506.16791},
	abstract = {With the growing popularity of deep learning and foundation models for tabular data, the need for standardized and reliable benchmarks is higher than ever. However, current benchmarks are static. Their design is not updated even if flaws are discovered, model versions are updated, or new models are released. To address this, we introduce TabArena, the first continuously maintained living tabular benchmarking system. To launch TabArena, we manually curate a representative collection of datasets and well-implemented models, conduct a large-scale benchmarking study to initialize a public leaderboard, and assemble a team of experienced maintainers. Our results highlight the influence of validation method and ensembling of hyperparameter configurations to benchmark models at their full potential. While gradient-boosted trees are still strong contenders on practical tabular datasets, we observe that deep learning methods have caught up under larger time budgets with ensembling. At the same time, foundation models excel on smaller datasets. Finally, we show that ensembles across models advance the state-of-the-art in tabular machine learning. We observe that some deep learning models are overrepresented in cross-model ensembles due to validation set overfitting, and we encourage model developers to address this issue. We launch TabArena with a public leaderboard, reproducible code, and maintenance protocols to create a living benchmark available at https://tabarena.ai.},
	urldate = {2025-11-12},
	publisher = {arXiv},
	author = {Erickson, Nick and Purucker, Lennart and Tschalzev, Andrej and Holzmüller, David and Desai, Prateek Mutalik and Salinas, David and Hutter, Frank},
	month = nov,
	year = {2025},
	note = {arXiv:2506.16791 [cs]
TLDR: This work introduces TabArena, the first continuously maintained living tabular benchmarking system, and shows that ensembles across models advance the state-of-the-art in tabular machine learning.},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted (spotlight) at NeurIPS 2025 Datasets and Benchmarks Track. v4: fixed links in comments. v3: NeurIPS camera-ready version. v2: fixed author list. 51 pages. Code available at https://tabarena.ai/code and examples at https://tabarena.ai/code-examples and dataset curation at https://tabarena.ai/data-tabular-ml-iid-study and https://tabarena.ai/dataset-curation},
	file = {Full Text PDF:/Users/tonywang/Zotero/storage/KSSS5JCD/Erickson et al. - 2025 - TabArena A Living Benchmark for Machine Learning on Tabular Data.pdf:application/pdf;Snapshot:/Users/tonywang/Zotero/storage/DGYGVP2G/2506.html:text/html},
}

@misc{gorishniy_tabm_2025,
	title = {{TabM}: {Advancing} {Tabular} {Deep} {Learning} with {Parameter}-{Efficient} {Ensembling}},
	shorttitle = {{TabM}},
	url = {http://arxiv.org/abs/2410.24210},
	doi = {10.48550/arXiv.2410.24210},
	abstract = {Deep learning architectures for supervised learning on tabular data range from simple multilayer perceptrons (MLP) to sophisticated Transformers and retrieval-augmented methods. This study highlights a major, yet so far overlooked opportunity for designing substantially better MLP-based tabular architectures. Namely, our new model TabM relies on efficient ensembling, where one TabM efficiently imitates an ensemble of MLPs and produces multiple predictions per object. Compared to a traditional deep ensemble, in TabM, the underlying implicit MLPs are trained simultaneously, and (by default) share most of their parameters, which results in significantly better performance and efficiency. Using TabM as a new baseline, we perform a large-scale evaluation of tabular DL architectures on public benchmarks in terms of both task performance and efficiency, which renders the landscape of tabular DL in a new light. Generally, we show that MLPs, including TabM, form a line of stronger and more practical models compared to attention- and retrieval-based architectures. In particular, we find that TabM demonstrates the best performance among tabular DL models. Then, we conduct an empirical analysis on the ensemble-like nature of TabM. We observe that the multiple predictions of TabM are weak individually, but powerful collectively. Overall, our work brings an impactful technique to tabular DL and advances the performance-efficiency trade-off with TabM -- a simple and powerful baseline for researchers and practitioners.},
	urldate = {2025-11-12},
	publisher = {arXiv},
	author = {Gorishniy, Yury and Kotelnikov, Akim and Babenko, Artem},
	month = feb,
	year = {2025},
	note = {arXiv:2410.24210 [cs]
TLDR: This work's new model TabM relies on efficient ensembling, where one TabM efficiently imitates an ensemble of MLPs and produces multiple predictions per object, and demonstrates the best performance among tabular DL models.},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: ICLR 2025. Code: https://github.com/yandex-research/tabm},
	file = {Snapshot:/Users/tonywang/Zotero/storage/X8WU838C/2410.html:text/html},
}

@misc{zhang_mitra_2025,
	title = {Mitra: {Mixed} {Synthetic} {Priors} for {Enhancing} {Tabular} {Foundation} {Models}},
	shorttitle = {Mitra},
	url = {http://arxiv.org/abs/2510.21204},
	doi = {10.48550/arXiv.2510.21204},
	abstract = {Since the seminal work of TabPFN, research on tabular foundation models (TFMs) based on in-context learning (ICL) has challenged long-standing paradigms in machine learning. Without seeing any real-world data, models pretrained on purely synthetic datasets generalize remarkably well across diverse datasets, often using only a moderate number of in-context examples. This shifts the focus in tabular machine learning from model architecture design to the design of synthetic datasets, or, more precisely, to the prior distributions that generate them. Yet the guiding principles for prior design remain poorly understood. This work marks the first attempt to address the gap. We systematically investigate and identify key properties of synthetic priors that allow pretrained TFMs to generalize well. Based on these insights, we introduce Mitra, a TFM trained on a curated mixture of synthetic priors selected for their diversity, distinctiveness, and performance on real-world tabular data. Mitra consistently outperforms state-of-the-art TFMs, such as TabPFNv2 and TabICL, across both classification and regression benchmarks, with better sample efficiency.},
	urldate = {2025-11-12},
	publisher = {arXiv},
	author = {Zhang, Xiyuan and Maddix, Danielle C. and Yin, Junming and Erickson, Nick and Ansari, Abdul Fatir and Han, Boran and Zhang, Shuai and Akoglu, Leman and Faloutsos, Christos and Mahoney, Michael W. and Hu, Cuixiong and Rangwala, Huzefa and Karypis, George and Wang, Bernie},
	month = oct,
	year = {2025},
	note = {arXiv:2510.21204 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: NeurIPS 2025. We released both classifier (autogluon/mitra-classifier) and regressor (autogluon/mitra-regressor) model weights on HuggingFace},
	file = {Snapshot:/Users/tonywang/Zotero/storage/NE3UHF27/2510.html:text/html},
}

@misc{erickson_autogluon-tabular_2020,
	title = {{AutoGluon}-{Tabular}: {Robust} and {Accurate} {AutoML} for {Structured} {Data}},
	shorttitle = {{AutoGluon}-{Tabular}},
	url = {http://arxiv.org/abs/2003.06505},
	doi = {10.48550/arXiv.2003.06505},
	abstract = {We introduce AutoGluon-Tabular, an open-source AutoML framework that requires only a single line of Python to train highly accurate machine learning models on an unprocessed tabular dataset such as a CSV file. Unlike existing AutoML frameworks that primarily focus on model/hyperparameter selection, AutoGluon-Tabular succeeds by ensembling multiple models and stacking them in multiple layers. Experiments reveal that our multi-layer combination of many models offers better use of allocated training time than seeking out the best. A second contribution is an extensive evaluation of public and commercial AutoML platforms including TPOT, H2O, AutoWEKA, auto-sklearn, AutoGluon, and Google AutoML Tables. Tests on a suite of 50 classification and regression tasks from Kaggle and the OpenML AutoML Benchmark reveal that AutoGluon is faster, more robust, and much more accurate. We find that AutoGluon often even outperforms the best-in-hindsight combination of all of its competitors. In two popular Kaggle competitions, AutoGluon beat 99\% of the participating data scientists after merely 4h of training on the raw data.},
	urldate = {2025-11-12},
	publisher = {arXiv},
	author = {Erickson, Nick and Mueller, Jonas and Shirkov, Alexander and Zhang, Hang and Larroy, Pedro and Li, Mu and Smola, Alexander},
	month = mar,
	year = {2020},
	note = {arXiv:2003.06505 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Full Text PDF:/Users/tonywang/Zotero/storage/CAU8LE8B/Erickson et al. - 2020 - AutoGluon-Tabular Robust and Accurate AutoML for Structured Data.pdf:application/pdf;Snapshot:/Users/tonywang/Zotero/storage/MB4YLZ73/2003.html:text/html},
}

@misc{qu_tabicl_2025,
	title = {{TabICL}: {A} {Tabular} {Foundation} {Model} for {In}-{Context} {Learning} on {Large} {Data}},
	shorttitle = {{TabICL}},
	url = {http://arxiv.org/abs/2502.05564},
	doi = {10.48550/arXiv.2502.05564},
	abstract = {The long-standing dominance of gradient-boosted decision trees on tabular data is currently challenged by tabular foundation models using In-Context Learning (ICL): setting the training data as context for the test data and predicting in a single forward pass without parameter updates. While TabPFNv2 foundation model excels on tables with up to 10K samples, its alternating column- and row-wise attentions make handling large training sets computationally prohibitive. So, can ICL be effectively scaled and deliver a benefit for larger tables? We introduce TabICL, a tabular foundation model for classification, pretrained on synthetic datasets with up to 60K samples and capable of handling 500K samples on affordable resources. This is enabled by a novel two-stage architecture: a column-then-row attention mechanism to build fixed-dimensional embeddings of rows, followed by a transformer for efficient ICL. Across 200 classification datasets from the TALENT benchmark, TabICL is on par with TabPFNv2 while being systematically faster (up to 10 times), and significantly outperforms all other approaches. On 53 datasets with over 10K samples, TabICL surpasses both TabPFNv2 and CatBoost, demonstrating the potential of ICL for large data. Pretraining code, inference code, and pre-trained models are available at https://github.com/soda-inria/tabicl.},
	urldate = {2025-11-12},
	publisher = {arXiv},
	author = {Qu, Jingang and Holzmüller, David and Varoquaux, Gaël and Morvan, Marine Le},
	month = may,
	year = {2025},
	note = {arXiv:2502.05564 [cs]
TLDR: TabICL, a tabular foundation model for classification, pretrained on synthetic datasets with up to 60K samples and capable of handling 500K samples on affordable resources, is introduced, demonstrating the potential of ICL for large data.},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Published at ICML 2025},
	file = {Snapshot:/Users/tonywang/Zotero/storage/DC2MV3B6/2502.html:text/html},
}

@inproceedings{ke_lightgbm_2017,
	title = {{LightGBM}: {A} {Highly} {Efficient} {Gradient} {Boosting} {Decision} {Tree}},
	volume = {30},
	shorttitle = {{LightGBM}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html},
	abstract = {Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: {\textbackslash}emph\{Gradient-based One-Side Sampling\} (GOSS) and {\textbackslash}emph\{Exclusive Feature Bundling\} (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB {\textbackslash}emph\{LightGBM\}. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.},
	urldate = {2025-11-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
	year = {2017},
	file = {Full Text PDF:/Users/tonywang/Zotero/storage/2DC9RC4I/Ke et al. - 2017 - LightGBM A Highly Efficient Gradient Boosting Decision Tree.pdf:application/pdf},
}

@inproceedings{prokhorenkova_catboost_2018,
	title = {{CatBoost}: unbiased boosting with categorical features},
	volume = {31},
	shorttitle = {{CatBoost}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/14491b756b3a51daac41c24863285549-Abstract.html},
	abstract = {This paper presents the key algorithmic techniques behind CatBoost, a new gradient boosting toolkit. Their combination leads to CatBoost outperforming other publicly available boosting implementations in terms of quality on a variety of datasets. Two critical algorithmic advances introduced in CatBoost are the implementation of ordered boosting, a permutation-driven alternative to the classic algorithm, and an innovative algorithm for processing categorical features. Both techniques were created to fight a prediction shift caused by a special kind of target leakage present in all currently existing implementations of gradient boosting algorithms. In this paper, we provide a detailed analysis of this problem and demonstrate that proposed algorithms solve it effectively, leading to excellent empirical results.},
	urldate = {2025-11-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
	year = {2018},
	file = {Full Text PDF:/Users/tonywang/Zotero/storage/QAZ6QY7H/Prokhorenkova et al. - 2018 - CatBoost unbiased boosting with categorical features.pdf:application/pdf},
}

@article{hollmann_accurate_2025,
	title = {Accurate predictions on small data with a tabular foundation model},
	volume = {637},
	copyright = {2025 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-08328-6},
	doi = {10.1038/s41586-024-08328-6},
	abstract = {Tabular data, spreadsheets organized in rows and columns, are ubiquitous across scientific fields, from biomedicine to particle physics to economics and climate science1,2. The fundamental prediction task of filling in missing values of a label column based on the rest of the columns is essential for various applications as diverse as biomedical risk models, drug discovery and materials science. Although deep learning has revolutionized learning from raw data and led to numerous high-profile success stories3–5, gradient-boosted decision trees6–9 have dominated tabular data for the past 20 years. Here we present the Tabular Prior-data Fitted Network (TabPFN), a tabular foundation model that outperforms all previous methods on datasets with up to 10,000 samples by a wide margin, using substantially less training time. In 2.8 s, TabPFN outperforms an ensemble of the strongest baselines tuned for 4 h in a classification setting. As a generative transformer-based foundation model, this model also allows fine-tuning, data generation, density estimation and learning reusable embeddings. TabPFN is a learning algorithm that is itself learned across millions of synthetic datasets, demonstrating the power of this approach for algorithm development. By improving modelling abilities across diverse fields, TabPFN has the potential to accelerate scientific discovery and enhance important decision-making in various domains.},
	language = {en},
	number = {8045},
	urldate = {2025-11-12},
	journal = {Nature},
	author = {Hollmann, Noah and Müller, Samuel and Purucker, Lennart and Krishnakumar, Arjun and Körfer, Max and Hoo, Shi Bin and Schirrmeister, Robin Tibor and Hutter, Frank},
	month = jan,
	year = {2025},
	note = {Publisher: Nature Publishing Group
TLDR: The Tabular Prior-data Fitted Network (TabPFN), a tabular foundation model, provides accurate predictions on small data and outperforms all previous methods on datasets with up to 10,000 samples by a wide margin, using substantially less training time.},
	keywords = {Computational science, Computer science, Scientific data, Software, Statistics},
	pages = {319--326},
	file = {Full Text PDF:/Users/tonywang/Zotero/storage/CM49RVR7/Hollmann et al. - 2025 - Accurate predictions on small data with a tabular foundation model.pdf:application/pdf},
}

@inproceedings{chen_xgboost_2016,
	address = {San Francisco California USA},
	title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {{XGBoost}},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939785},
	doi = {10.1145/2939672.2939785},
	abstract = {Tree boosting is a highly eﬀective and widely used machine learning method. In this paper, we describe a scalable endto-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
	language = {en},
	urldate = {2025-11-12},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Chen, Tianqi and Guestrin, Carlos},
	month = aug,
	year = {2016},
	note = {TLDR: This paper proposes a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning and provides insights on cache access patterns, data compression and sharding to build a scalable tree boosting system called XGBoost.},
	pages = {785--794},
	file = {PDF:/Users/tonywang/Zotero/storage/VNHKB54M/Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf:application/pdf},
}

@misc{erickson_autogluonautogluon_2025,
	title = {autogluon/autogluon},
	copyright = {Apache-2.0},
	url = {https://github.com/autogluon/autogluon},
	abstract = {Fast and Accurate ML in 3 Lines of Code},
	urldate = {2025-11-12},
	publisher = {autogluon},
	author = {Erickson, Nick},
	month = nov,
	year = {2025},
	note = {original-date: 2019-07-29T18:51:24Z},
	keywords = {deep-learning, machine-learning, tabular-data, autogluon, automated-machine-learning, automl, computer-vision, data-science, ensemble-learning, forecasting, gluon, hyperparameter-optimization, natural-language-processing, object-detection, python, pytorch, scikit-learn, structured-data, time-series, transfer-learning},
}

@article{breiman_random_2001,
	title = {Random {Forests}},
	volume = {45},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	language = {en},
	number = {1},
	urldate = {2025-11-12},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = oct,
	year = {2001},
	note = {TLDR: Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the forest, and are also applicable to regression.},
	keywords = {classification, ensemble, regression},
	pages = {5--32},
	file = {Full Text PDF:/Users/tonywang/Zotero/storage/D5ZRJX2S/Breiman - 2001 - Random Forests.pdf:application/pdf},
}

@inproceedings{drucker_support_1996,
	title = {Support {Vector} {Regression} {Machines}},
	volume = {9},
	url = {https://proceedings.neurips.cc/paper_files/paper/1996/hash/d38901788c533e8286cb6400b40b386d-Abstract.html},
	abstract = {A  new  regression  technique  based  on  Vapnik's  concept  of  support  vectors  is  introduced.  We compare  support  vector  regression  (SVR)  with  a  committee regression  technique  (bagging)  based  on  regression  trees  and  ridge regression  done in  feature space.  On  the basis of these  experiments,  it  is  expected  that  SVR  will  have  advantages  in  high  dimensionality space because SVR optimization does not depend on the  dimensionality of the input space.},
	urldate = {2025-11-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Drucker, Harris and Burges, Christopher J. C. and Kaufman, Linda and Smola, Alex and Vapnik, Vladimir},
	year = {1996},
	file = {Full Text PDF:/Users/tonywang/Zotero/storage/WDZGEQHQ/Drucker et al. - 1996 - Support Vector Regression Machines.pdf:application/pdf},
}

@article{cover_nearest_1967,
	title = {Nearest neighbor pattern classification},
	volume = {13},
	issn = {1557-9654},
	url = {https://ieeexplore.ieee.org/document/1053964},
	doi = {10.1109/TIT.1967.1053964},
	abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of errorRof such a rule must be at least as great as the Bayes probability of errorR{\textasciicircum}{\textbackslash}ast–the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in theM-category case thatR{\textasciicircum}{\textbackslash}ast łeq R łeq R{\textasciicircum}{\textbackslash}ast(2 –MR{\textasciicircum}{\textbackslash}ast/(M-1)), where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
	number = {1},
	urldate = {2025-11-12},
	journal = {IEEE Transactions on Information Theory},
	author = {Cover, T. and Hart, P.},
	month = jan,
	year = {1967},
	note = {TLDR: The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points, so it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
	keywords = {Accuracy, Auditory displays, Bayes methods, Convergence, Density measurement, Extraterrestrial measurements, Loss measurement, Posterior probability, Random variables, Visualization},
	pages = {21--27},
}

@article{yeo_new_2000,
	title = {A new family of power transformations to improve normality or symmetry},
	volume = {87},
	issn = {0006-3444},
	url = {https://dx.doi.org/10.1093/biomet/87.4.954},
	doi = {10.1093/biomet/87.4.954},
	abstract = {Abstract. We introduce a new power transformation family which is well defined on the whole real line and which is appropriate for reducing skewness and to},
	language = {en},
	number = {4},
	urldate = {2025-11-12},
	journal = {Biometrika},
	author = {Yeo, In-Kwon and Johnson, Richard A.},
	month = dec,
	year = {2000},
	note = {Publisher: Oxford Academic},
	pages = {954--959},
}

@misc{zhang_limix_2025,
	title = {{LimiX}: {Unleashing} {Structured}-{Data} {Modeling} {Capability} for {Generalist} {Intelligence}},
	shorttitle = {{LimiX}},
	url = {http://arxiv.org/abs/2509.03505},
	doi = {10.48550/arXiv.2509.03505},
	abstract = {We argue that progress toward general intelligence requires complementary foundation models grounded in language, the physical world, and structured data. This report presents LimiX-16M and LimiX-2M, two instantiations of our large structured-data models (LDMs). Both models treat structured data as a joint distribution over variables and missingness, thus capable of addressing a wide range of tabular tasks through query-based conditional prediction via a single model. They are pretrained using masked joint-distribution modeling with an episodic, context-conditional objective, supporting rapid, training-free adaptation at inference. We evaluate LimiX models across 11 large structured-data benchmarks with broad regimes of sample size, feature dimensionality, class number, categorical-to-numerical feature ratio, missingness, and sample-to-feature ratios. LimiX-16M consistently surpasses strong baselines, as shown in Figure 1 and Figure 2. The superiority holds across a wide range of tasks, such as classification, regression, missing value imputation, and data generation, often by substantial margins, while avoiding task-specific architectures or bespoke training per task. Notably, LimiX-2M delivers strong results under tight compute and memory budgets. We also present the first scaling law study for LDMs, revealing how data and model scaling jointly influence downstream performance and offering quantitative guidance for tabular foundation modeling. All LimiX models are publicly accessible under Apache 2.0.},
	urldate = {2025-12-18},
	publisher = {arXiv},
	author = {Zhang, Xingxuan and Ren, Gang and Yu, Han and Yuan, Hao and Wang, Hui and Li, Jiansheng and Wu, Jiayun and Mo, Lang and Mao, Li and Hao, Mingchao and Dai, Ningbo and Xu, Renzhe and Li, Shuyang and Zhang, Tianyang and He, Yue and Wang, Yuanrui and Zhang, Yunjia and Xu, Zijing and Li, Dongzhe and Gao, Fang and Zou, Hao and Liu, Jiandong and Liu, Jiashuo and Xu, Jiawei and Cheng, Kaijie and Li, Kehan and Zhou, Linjun and Li, Qing and Fan, Shaohua and Lin, Xiaoyu and Han, Xinyan and Li, Xuanyue and Lu, Yan and Xue, Yuan and Jiang, Yuanyuan and Wang, Zimu and Wang, Zhenlei and Cui, Peng},
	month = nov,
	year = {2025},
	note = {arXiv:2509.03505 [cs]
TLDR: LimiX-16M and LimiX-2M are presented, two instantiations of the authors' large structured-data models that treat structured data as a joint distribution over variables and missingness, thus capable of addressing a wide range of tabular tasks through query-based conditional prediction via a single model.},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 61 pages},
	file = {Preprint PDF:/Users/tonywang/Zotero/storage/MHPXRJRM/Zhang et al. - 2025 - LimiX Unleashing Structured-Data Modeling Capability for Generalist Intelligence.pdf:application/pdf;Snapshot:/Users/tonywang/Zotero/storage/GFGURGI5/2509.html:text/html},
}

@misc{world_health_organization_cardiovascular_2025,
	title = {Cardiovascular diseases ({CVDs})},
	copyright = {© 2025 World Health Organization},
	shorttitle = {Cardiovascular diseases},
	url = {https://www.who.int/news-room/fact-sheets/detail/cardiovascular-diseases-(cvds)},
	abstract = {WHO cardiovascular diseases fact sheet providing key facts and information on risk factors, symptoms, rheumatic heart disease, treatment and prevention, WHO response.},
	language = {en},
	urldate = {2025-12-18},
	journal = {World Health Organization},
	author = {{World Health Organization}},
	month = jul,
	year = {2025},
	file = {Snapshot:/Users/tonywang/Zotero/storage/5EDK66V2/cardiovascular-diseases-(cvds).html:text/html},
}

@misc{ismail_heart_nodate,
	title = {Heart {Attack} {Analysis} \& {Prediction} {Dataset}},
	copyright = {CC0: Public Domain},
	shorttitle = {Heart {Attack} {Analysis}},
	url = {https://www.kaggle.com/datasets/sonialikhan/heart-attack-analysis-and-prediction-dataset},
	abstract = {A dataset for heart attack classification},
	language = {en},
	urldate = {2025-12-18},
	author = {Ismail, Hina and AbdulWahab, Kabani and Shahane, Saurabh and Ahmed, Zohaib and {Zoey} and Khaled, Abdulrahman and Ahmed, Zohair and {Sheikh Muhamad Abdullah}},
	file = {Snapshot:/Users/tonywang/Zotero/storage/LZEYKBLK/heart-attack-analysis-and-prediction-dataset.html:text/html},
}

@misc{pavlovic_understanding_2025,
	title = {Understanding {Model} {Calibration} -- {A} gentle introduction and visual exploration of calibration and the expected calibration error ({ECE})},
	url = {http://arxiv.org/abs/2501.19047},
	doi = {10.48550/arXiv.2501.19047},
	abstract = {To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to be an in-depth dissection of all works on calibration, nor does it focus on how to calibrate models. Instead, it is meant to provide a gentle introduction to the different notions and their evaluation measures as well as to re-highlight some issues with a measure that is still widely used to evaluate calibration.},
	urldate = {2025-12-19},
	publisher = {arXiv},
	author = {Pavlovic, Maja},
	month = feb,
	year = {2025},
	note = {arXiv:2501.19047 [stat]
version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	file = {Full Text PDF:/Users/tonywang/Zotero/storage/ZL3BZKWF/Pavlovic - 2025 - Understanding Model Calibration -- A gentle introduction and visual exploration of calibration and t.pdf:application/pdf;Snapshot:/Users/tonywang/Zotero/storage/4KW59H3R/2501.html:text/html},
}

@misc{guo_calibration_2017,
	title = {On {Calibration} of {Modern} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1706.04599},
	doi = {10.48550/arXiv.1706.04599},
	abstract = {Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.},
	urldate = {2025-12-19},
	publisher = {arXiv},
	author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
	month = aug,
	year = {2017},
	note = {arXiv:1706.04599 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: ICML 2017},
	file = {Full Text PDF:/Users/tonywang/Zotero/storage/XD6K73LW/Guo et al. - 2017 - On Calibration of Modern Neural Networks.pdf:application/pdf;Snapshot:/Users/tonywang/Zotero/storage/D53CLJPW/1706.html:text/html},
}

@misc{popordanoska_consistent_2022,
	title = {A {Consistent} and {Differentiable} {Lp} {Canonical} {Calibration} {Error} {Estimator}},
	url = {http://arxiv.org/abs/2210.07810},
	doi = {10.48550/arXiv.2210.07810},
	abstract = {Calibrated probabilistic classifiers are models whose predicted probabilities can directly be interpreted as uncertainty estimates. It has been shown recently that deep neural networks are poorly calibrated and tend to output overconfident predictions. As a remedy, we propose a low-bias, trainable calibration error estimator based on Dirichlet kernel density estimates, which asymptotically converges to the true \$L\_p\$ calibration error. This novel estimator enables us to tackle the strongest notion of multiclass calibration, called canonical (or distribution) calibration, while other common calibration methods are tractable only for top-label and marginal calibration. The computational complexity of our estimator is \${\textbackslash}mathcal\{O\}(n{\textasciicircum}2)\$, the convergence rate is \${\textbackslash}mathcal\{O\}(n{\textasciicircum}\{-1/2\})\$, and it is unbiased up to \${\textbackslash}mathcal\{O\}(n{\textasciicircum}\{-2\})\$, achieved by a geometric series debiasing scheme. In practice, this means that the estimator can be applied to small subsets of data, enabling efficient estimation and mini-batch updates. The proposed method has a natural choice of kernel, and can be used to generate consistent estimates of other quantities based on conditional expectation, such as the sharpness of a probabilistic classifier. Empirical results validate the correctness of our estimator, and demonstrate its utility in canonical calibration error estimation and calibration error regularized risk minimization.},
	urldate = {2025-12-19},
	publisher = {arXiv},
	author = {Popordanoska, Teodora and Sayer, Raphael and Blaschko, Matthew B.},
	month = oct,
	year = {2022},
	note = {arXiv:2210.07810 [stat]
TLDR: A low-bias, trainable calibration error estimator based on Dirichlet kernel density estimates, which asymptotically converges to the true \$L\_p\$ calibration error, which enables this novel estimator to tackle the strongest notion of multiclass calibration, called canonical calibration.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	annote = {Comment: To appear at NeurIPS 2022},
	file = {Preprint PDF:/Users/tonywang/Zotero/storage/P88JY7MH/Popordanoska et al. - 2022 - A Consistent and Differentiable Lp Canonical Calibration Error Estimator.pdf:application/pdf},
}

@article{atzmueller_subgroup_2015,
	title = {Subgroup discovery},
	volume = {5},
	issn = {1942-4795},
	url = {https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.1144},
	doi = {10.1002/widm.1144},
	abstract = {Subgroup discovery is a broadly applicable descriptive data mining technique for identifying interesting subgroups according to some property of interest. This article summarizes fundamentals of subg...},
	language = {en},
	number = {1},
	urldate = {2025-12-19},
	journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
	author = {Atzmueller, Martin},
	month = jan,
	year = {2015},
	note = {Publisher: John Wiley \& Sons, Ltd},
	pages = {35--49},
}

@incollection{lemmerich_pysubgroup_2019,
	title = {pysubgroup: {Easy}-to-{Use} {Subgroup} {Discovery} in {Python}: {European} {Conference}, {ECML} {PKDD} 2018, {Dublin}, {Ireland}, {September} 10–14, 2018, {Proceedings}, {Part} {III}},
	isbn = {978-3-030-10996-7},
	shorttitle = {pysubgroup},
	abstract = {This paper introduces the pysubgroup package for subgroup discovery in Python. Subgroup discovery is a well-established data mining task that aims at identifying describable subsets in the data that show an interesting distribution with respect to a certain target concept. The presented package provides an easy-to-use, compact and extensible implementation of state-of-the-art mining algorithms, interestingness measures, and visualizations. Since it builds directly on the established pandas data analysis library—a de-facto standard for data science in Python—it seamlessly integrates into preprocessing and exploratory data analysis steps. Code related to this paper is available at: http://florian.lemmerich.net/pysubgroup.},
	author = {Lemmerich, Florian and Becker, Martin},
	month = jan,
	year = {2019},
	doi = {10.1007/978-3-030-10997-4_46},
	pages = {658--662},
}
@article{cohn_silent_1988,
  title = {Silent Myocardial Ischemia},
  author = {Cohn, Peter F.},
  journal = {Annals of Internal Medicine},
  volume = {109},
  number = {4},
  pages = {312--317},
  year = {1988},
  doi = {10.7326/0003-4819-109-4-312}
}

@article{cohn_silent_2003,
  title = {Silent Myocardial Ischemia},
  author = {Cohn, Peter F. and Fox, Kim M. and Daly, Catriona},
  journal = {Circulation},
  volume = {108},
  number = {10},
  pages = {1263--1277},
  year = {2003},
  doi = {10.1161/01.CIR.0000088001.59265.EE}
}

@article{gottlieb_silent_1986,
  title = {Silent Ischemia as a Marker for Early Unfavorable Outcomes in Patients with Unstable Angina},
  author = {Gottlieb, Sheldon O. and Weisfeldt, Myron L. and Ouyang, Pamela and Mellits, E. David and Gerstenblith, Gary},
  journal = {New England Journal of Medicine},
  volume = {314},
  number = {19},
  pages = {1214--1219},
  year = {1986},
  doi = {10.1056/NEJM198605083141903}
}

@article{lim_st_2016,
  title = {{ST}-Segment Changes with Exercise Stress},
  author = {Lim, Yoke Ching and Teo, Swee-Guan and Poh, Kian-Keong},
  journal = {Singapore Medical Journal},
  volume = {57},
  number = {7},
  pages = {347--353},
  year = {2016},
  doi = {10.11622/smedj.2016116}
}

@article{gibbons_acc_2002,
  title = {{ACC/AHA} 2002 Guideline Update for Exercise Testing: Summary Article},
  author = {Gibbons, Raymond J. and Balady, Gary J. and Bricker, John Timothy and Chaitman, Bernard R. and Fletcher, Gerald F. and Froelicher, Victor F. and Mark, Daniel B. and McCallister, Ben D. and Mooss, Aryan N. and O'Reilly, Michael G. and Winters, William L.},
  journal = {Circulation},
  volume = {106},
  number = {14},
  pages = {1883--1892},
  year = {2002},
  doi = {10.1161/01.CIR.0000034670.06526.15}
}

@article{greenland_cac_2018,
  title = {Coronary Calcium Score and Cardiovascular Risk},
  author = {Greenland, Philip and Blaha, Michael J. and Budoff, Matthew J. and Erbel, Raimund and Watson, Karol E.},
  journal = {Journal of the American College of Cardiology},
  volume = {72},
  number = {4},
  pages = {434--447},
  year = {2018},
  doi = {10.1016/j.jacc.2018.05.027}
}

@article{mosca_sex_2011,
  title = {Sex/Gender Differences in Cardiovascular Disease Prevention: What a Difference a Decade Makes},
  author = {Mosca, Lori and Barrett-Connor, Elizabeth and Wenger, Nanette Kass},
  journal = {Circulation},
  volume = {124},
  number = {19},
  pages = {2145--2154},
  year = {2011},
  doi = {10.1161/CIRCULATIONAHA.110.968792}
}